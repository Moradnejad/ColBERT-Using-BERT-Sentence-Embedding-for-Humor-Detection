{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import subprocess\nfrom ast import literal_eval\n\ndef run(command):\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    print(out.decode('utf-8').strip())\n\nprint('# CPU')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^model name\"')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^cpu MHz\"')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^cpu cores\"')\n\nprint('# RAM')\nrun('cat /proc/meminfo | egrep \"^MemTotal\"')\n\nprint('# GPU')\nrun('lspci | grep VGA')\n\nprint('# OS')\nrun('uname -a')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n# import tensorflow_hub as hub\nimport tensorflow as tf\n# import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nimport seaborn as sns\nimport string\nimport re    #for regex\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choose model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\n\nMODEL_TYPE = 'bert-base-uncased'\nMAX_SIZE = 200\nBATCH_SIZE = 200\n\ntokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"},{"metadata":{"trusted":true},"cell_type":"code","source":"HAS_ANS = False\ntraining_sample_count = 1000 # 4000\ntraining_epochs = 1 # 3\ntest_count = 1000\nrunning_folds = 1 # 2\nMAX_SEQUENCE_LENGTH = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/200k-short-texts-for-humor-detection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/cbert-after-preprocessing-by-1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### original dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/dataset.csv')\n\ndf_train = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/train.csv')\ndisplay(df_train.head(3))\ndf_train = df_train[:training_sample_count*5]\n\ndf_test = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/dev.csv')\ndisplay(df_test.head(3))\ndf_test = df_test[:test_count]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_y = df_test.copy()\ndel df_test['humor']\n\ndf_sub = test_df_y.copy()\n\nprint(len(df),len(df_train),len(df_test))\ndisplay(df_train.head())\ndisplay(df_test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(df_train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_categories = list(df_train.columns[[1]])\ninput_categories = list(df_train.columns[[0]])\n\nif HAS_ANS:\n    output_categories = list(df_train.columns[11:])\n    input_categories = list(df_train.columns[[1,2,5]])\n    \n\nTARGET_COUNT = len(output_categories)\n\nprint('\\ninput categories:\\n\\t', input_categories)\nprint('\\noutput TARGET_COUNT:\\n\\t', TARGET_COUNT)\nprint('\\noutput categories:\\n\\t', output_categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n\n*update 4:* credits to [Minh](https://www.kaggle.com/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input, it will require significantly more memory when finetuning BERT."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        '', None, 'longest_first', max_sequence_length)\n        \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.text, instance.text, instance.text\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create model\n\n~~`compute_spearmanr()`~~ `mean_squared_error` is used to compute the competition metric for the validation set\n<br><br>\n`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n#     x = tf.keras.layers.Concatenate()([q, q])\n    \n    x = tf.keras.layers.Dropout(0.2)(q)\n    \n    x = tf.keras.layers.Dense(TARGET_COUNT, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, ], outputs=x)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Obtain inputs and targets, as well as the indices of the train/validation splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"output_categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nimport sklearn\ndef print_evaluation_metrics(y_true, y_pred, label='', is_regression=True, label2=''):\n    print('==================', label2)\n    ### For regression\n    if is_regression:\n        print('mean_absolute_error',label,':', sklearn.metrics.mean_absolute_error(y_true, y_pred))\n        print('mean_squared_error',label,':', sklearn.metrics.mean_squared_error(y_true, y_pred))\n        print('r2 score',label,':', sklearn.metrics.r2_score(y_true, y_pred))\n        #     print('max_error',label,':', sklearn.metrics.max_error(y_true, y_pred))\n        return sklearn.metrics.mean_squared_error(y_true, y_pred)\n    else:\n        ### FOR Classification\n        print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n        print('average_precision_score',label,':', sklearn.metrics.average_precision_score(y_true, y_pred))\n        print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n        print('accuracy_score',label,':', sklearn.metrics.accuracy_score(y_true, y_pred))\n        print('f1_score',label,':', sklearn.metrics.f1_score(y_true, y_pred))\n        \n        matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n        print(matrix)\n        TP,TN,FP,FN = matrix[1][1],matrix[0][0],matrix[0][1],matrix[1][0]\n        Accuracy = (TP+TN)/(TP+FP+FN+TN)\n        Precision = TP/(TP+FP)\n        Recall = TP/(TP+FN)\n        F1 = 2*(Recall * Precision) / (Recall + Precision)\n        print('Acc', Accuracy, 'Prec', Precision, 'Rec', Recall, 'F1',F1)\n        return sklearn.metrics.accuracy_score(y_true, y_pred)\n\nprint_evaluation_metrics([1,0], [0.9,0.1], '', True)\nprint_evaluation_metrics([1,0], [1,1], '', False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss function selection\nRegression problem between 0 and 1, so binary_crossentropy and mean_absolute_error seem good.\n\nHere are the explanations: https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_acc = 1000000\nmin_test = []\nvalid_preds = []\ntest_preds = []\nbest_model = False\nfor LR in np.arange(1e-5, 2e-5, 3e-5).tolist():\n    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n    print('LR=', LR)\n    gkf = GroupKFold(n_splits=5).split(X=df_train.text, groups=df_train.text)\n\n    for fold, (train_idx, valid_idx) in enumerate(gkf):\n        if fold not in range(running_folds):\n            continue\n        train_inputs = [(inputs[i][train_idx])[:training_sample_count] for i in range(len(inputs))]\n        train_outputs = (outputs[train_idx])[:training_sample_count]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n\n        print(np.array(train_inputs).shape, np.array(train_outputs).shape)\n#         print(train_idx[:10], valid_idx[:10])\n\n        K.clear_session()\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        for xx in range(1):\n            model.fit(train_inputs, train_outputs, epochs=training_epochs, batch_size=6)\n            # model.save_weights(f'bert-{fold}.h5')\n            valid_preds.append(model.predict(valid_inputs))\n            #rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n            #print('validation score = ', rho_val)\n            acc = print_evaluation_metrics(np.array(valid_outputs), np.array(valid_preds[-1]), 'on #'+str(xx+1))\n            if acc < min_acc:\n                print('new acc >> ', acc)\n                min_acc = acc\n                best_model = model\n#                 min_test = model.predict(test_inputs)\n#                 test_preds.append(min_test)\n            print(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('best acc >> ', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(valid_inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(valid_outputs.shape, valid_preds[-1].shape)\nprint_evaluation_metrics(np.array(valid_outputs), np.array(valid_preds[-1]), '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\nmin_test = best_model.predict(test_inputs)\n\n## use min_test\n# min_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.to_csv('df_test.csv')\ntest_df_y.to_csv('test_df_y.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regression submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = test_df_y.copy()\n# df_sub['pred'] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\ndf_sub['pred'] = min_test\n\n\nprint_evaluation_metrics(df_sub['humor'], df_sub['pred'], '', True)\n\n\ndf_sub.to_csv('sub1.csv', index=False)\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Binary submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"for split in np.arange(0.1, 0.99, 0.1).tolist():\n    df_sub['pred_bi'] = (df_sub['pred'] > split)\n\n    print_evaluation_metrics(df_sub['humor'], df_sub['pred_bi'], '', False, 'SPLIT on '+str(split))\n\n    df_sub.to_csv('sub3.csv', index=False)\n    df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
